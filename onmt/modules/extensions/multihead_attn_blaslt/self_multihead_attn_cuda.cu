#include <vector>
#include <math.h>
#include <iostream>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <cuda_profiler_api.h>
#include <fstream>
#include <exception>
#include <memory>


#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>
#include <torch/extension.h>

#include "softmax_apex.h"
#include "gemm_blaslt.cuh"
#include "strided_batched_gemm_blaslt.cuh"

// symbol to be automatically resolved by PyTorch libs
// extern THCState *state;

namespace multihead_attn {
namespace self_bias {
namespace cublaslt {

std::vector<torch::Tensor> fwd_cuda(
                               bool                 use_time_mask,
							   bool                 is_training,
                               int                  heads,
                               torch::Tensor const& inputs,
                               torch::Tensor const& input_weights,
                               torch::Tensor const& output_weights,
                               torch::Tensor const& input_biases,
                               torch::Tensor const& output_biases,
                               torch::Tensor const& pad_mask,
                               float                dropout_prob,
                               torch::Tensor lt_workspace)

{
  int   embed_dim      = inputs.size(2);
  int   sequences      = inputs.size(1);
  int   q_seq_len      = inputs.size(0);
  int   k_seq_len      = q_seq_len;
  int   batches        = sequences * q_seq_len;
  int   head_dim       = embed_dim / heads;
  int   output_lin_dim = 3 * embed_dim;
  int   attn_batches   = heads * sequences;
  int   lead_dim       = attn_batches * 3 * head_dim;
  int   batch_stride   = 3 * head_dim;
  int   dropout_elems  = attn_batches * q_seq_len * k_seq_len;
  const float alpha          = 1.0;
  const float beta_zero      = 0.0;
  const float beta_one       = 1.0;
  const float scale          = 1.0 / sqrt(static_cast<float>(head_dim));
  const half halpha = __float2half_rn(alpha);
  const half hbeta_zero = __float2half_rn(beta_zero);
  const half hbeta_one = __float2half_rn(beta_one);
  const half hscale = __float2half_rn(scale);

  // There is no reason to use more than one stream as every kernel is
  // sequentially dependent
  cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();
  cudaStream_t   stream = at::cuda::getCurrentCUDAStream().stream();
  cublasSetStream(handle, stream);

  // 3 Intermediate Results + Output (Note: dropout intermediates are generated by ATen library code)
  auto act_options  = inputs.options().requires_grad(false);
  auto mask_options = act_options.dtype(torch::kUInt8);

  torch::Tensor input_lin_results = torch::empty({q_seq_len, sequences, output_lin_dim}, act_options);
  torch::Tensor attn_scores       = torch::empty({attn_batches, q_seq_len, k_seq_len},      act_options);
  torch::Tensor dropout_results   = torch::empty({attn_batches, q_seq_len, k_seq_len},   act_options);
  torch::Tensor dropout_mask      = torch::empty({attn_batches, q_seq_len, k_seq_len},   mask_options);
  torch::Tensor matmul2_results   = torch::empty({q_seq_len, attn_batches, head_dim},    act_options);
  torch::Tensor outputs           = torch::empty_like(inputs, act_options);

  // Input Linear Results Pointers to Q, K, and V of interviewed activations
  void* q_lin_results_ptr   = static_cast<void*>(input_lin_results.data_ptr());
  void* k_lin_results_ptr   = static_cast<void*>(static_cast<half*>(input_lin_results.data_ptr()) + head_dim);
  void* v_lin_results_ptr   = static_cast<void*>(static_cast<half*>(input_lin_results.data_ptr()) + 2*head_dim);

  // Softmax Intermediate Result Ptr (used by Matmul1 -> Softmax)
  void* attn_scores_ptr = static_cast<void*>(attn_scores.data_ptr());
  void* dropout_results_ptr = static_cast<void*>(dropout_results.data_ptr());

  void* lt_workspace_ptr = static_cast<void*>(lt_workspace.data_ptr());

  TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));
  // Input Linear Fwd

  int cublas_status = 1;
  cublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            output_lin_dim,
            batches,
            embed_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(input_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(inputs.data_ptr()),
            embed_dim,
            &beta_zero, /* host pointer */
            q_lin_results_ptr,
            output_lin_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            true,
            static_cast<const void*>(input_biases.data_ptr()));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM QKV forward failed with %d\n", cublas_status);
      exit(0);
  }

  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            k_seq_len,
            q_seq_len,
            head_dim,
            &scale, /* host pointer */
            static_cast<const void*>(k_lin_results_ptr),
            lead_dim,
            batch_stride,
            static_cast<const void*>(q_lin_results_ptr),
            lead_dim,
            batch_stride,
            &beta_zero, /* host pointer */
            static_cast<void*>(attn_scores_ptr), // C
            k_seq_len, // ldc
            k_seq_len*q_seq_len, // stride c
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM QKV forward failed with %d\n", cublas_status);
      exit(0);
  }

  if (use_time_mask){
    attn_scores.masked_fill_(pad_mask, -std::numeric_limits<float>::infinity());
  } else {
    attn_scores.view({sequences, heads, q_seq_len, k_seq_len}).masked_fill_(pad_mask,
                                                                          -std::numeric_limits<float>::infinity());
  }
  // Padded Softmax
  bool softmax_success = false;

  if (is_training && dropout_prob > 0.0f) {
      // This function fuses softmax-dropout-pad (and dropout inplace)
      softmax_success = dispatch_softmax_dropout<half, half, float>(
                           reinterpret_cast<half*>(dropout_results_ptr),
                           (is_training) ? reinterpret_cast<uint8_t*>(dropout_mask.data_ptr<uint8_t>()) : nullptr,
                           reinterpret_cast<const half*>(attn_scores_ptr),
      		               dropout_elems,
                           k_seq_len,
                           k_seq_len,
                           attn_batches*q_seq_len,
      		               1.0f-dropout_prob,
		                   stream);
  } else {
      softmax_success = dispatch_softmax<half, half, float>(
                             reinterpret_cast<half*>(dropout_results_ptr), // this is actually softmax results, but making it consistent for the next function
                             reinterpret_cast<const half*>(attn_scores_ptr),
                             k_seq_len,
                             k_seq_len,
                             attn_batches*q_seq_len,
                             stream);  // pad batch strides
  }

  assert(softmax_success);

  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            head_dim,
            q_seq_len,
            k_seq_len,
            &alpha, /* host pointer */
            static_cast<const void*>(v_lin_results_ptr),  // A:
            lead_dim,
            batch_stride,
            static_cast<const void*>(dropout_results.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta_zero, /* host pointer */
            static_cast<void*>(matmul2_results.data_ptr()), // C
            head_dim*attn_batches,
            head_dim,
            attn_batches,
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM Context forward failed with %d\n", cublas_status);
      exit(0);
  }

  cublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            embed_dim,
            batches,
            embed_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(output_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(matmul2_results.data_ptr()),
            embed_dim,
            &beta_zero, /* host pointer */
            static_cast<void*>(outputs.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            true,
            static_cast<const void*>(output_biases.data_ptr()));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM output forward failed with %d\n", cublas_status);
      exit(0);
  }
  TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));

  return {
           input_lin_results,
           attn_scores,
           dropout_results,
           dropout_mask,
           matmul2_results,
           outputs
         };
}

// For relative or ALIBI
std::vector<torch::Tensor> fwd_bias_cuda(
                               bool                 use_time_mask,
							   bool                 is_training,
                               int                  heads,
                               torch::Tensor const& inputs,
                               torch::Tensor const& input_weights,
                               torch::Tensor const& output_weights,
                               torch::Tensor const& input_biases,
                               torch::Tensor const& output_biases,
                               torch::Tensor const& pad_mask,
                               torch::Tensor & bias,
                               float                dropout_prob,
                               torch::Tensor lt_workspace)

{
  int   embed_dim      = inputs.size(2);
  int   sequences      = inputs.size(1);
  int   q_seq_len      = inputs.size(0);
  int   k_seq_len      = q_seq_len;
  int   batches        = sequences * q_seq_len;
  int   head_dim       = embed_dim / heads;
  int   output_lin_dim = 3 * embed_dim;
  int   attn_batches   = heads * sequences;
  int   lead_dim       = attn_batches * 3 * head_dim;
  int   batch_stride   = 3 * head_dim;
  int   dropout_elems  = attn_batches * q_seq_len * k_seq_len;
  const float alpha          = 1.0;
  const float beta_zero      = 0.0;
  const float beta_one       = 1.0;
  const float scale          = 1.0 / sqrt(static_cast<float>(head_dim));
  const half halpha = __float2half_rn(alpha);
  const half hbeta_zero = __float2half_rn(beta_zero);
  const half hbeta_one = __float2half_rn(beta_one);
  const half hscale = __float2half_rn(scale);

  // There is no reason to use more than one stream as every kernel is
  // sequentially dependent
  cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();
  cudaStream_t   stream = at::cuda::getCurrentCUDAStream().stream();
  cublasSetStream(handle, stream);

  // 3 Intermediate Results + Output (Note: dropout intermediates are generated by ATen library code)
  auto act_options  = inputs.options().requires_grad(false);
  auto mask_options = act_options.dtype(torch::kUInt8);

  torch::Tensor input_lin_results = torch::empty({q_seq_len, sequences, output_lin_dim}, act_options);
//   torch::Tensor attn_scores       = torch::empty({attn_batches, q_seq_len, k_seq_len},      act_options);
  torch::Tensor attn_scores = bias;
  torch::Tensor dropout_results   = torch::empty({attn_batches, q_seq_len, k_seq_len},   act_options);
  torch::Tensor dropout_mask      = torch::empty({attn_batches, q_seq_len, k_seq_len},   mask_options);
  torch::Tensor matmul2_results   = torch::empty({q_seq_len, attn_batches, head_dim},    act_options);
  torch::Tensor outputs           = torch::empty_like(inputs, act_options);

  // Input Linear Results Pointers to Q, K, and V of interviewed activations
  void* q_lin_results_ptr   = static_cast<void*>(input_lin_results.data_ptr());
  void* k_lin_results_ptr   = static_cast<void*>(static_cast<half*>(input_lin_results.data_ptr()) + head_dim);
  void* v_lin_results_ptr   = static_cast<void*>(static_cast<half*>(input_lin_results.data_ptr()) + 2*head_dim);

  // Softmax Intermediate Result Ptr (used by Matmul1 -> Softmax)
  void* attn_scores_ptr = static_cast<void*>(attn_scores.data_ptr());
//   void* bias_ptr = static_cast<void*>(bias.data_ptr());
  void* dropout_results_ptr = static_cast<void*>(dropout_results.data_ptr());

  void* lt_workspace_ptr = static_cast<void*>(lt_workspace.data_ptr());

  TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));

  // Input Linear Fwd
  int cublas_status = 1;
  cublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            output_lin_dim,
            batches,
            embed_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(input_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(inputs.data_ptr()),
            embed_dim,
            &beta_zero, /* host pointer */
            q_lin_results_ptr,
            output_lin_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            true,
            static_cast<const void*>(input_biases.data_ptr()));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM QKV forward failed with %d\n", cublas_status);
      exit(0);
  }

  // fused QK GEMM and bias
  // here we do bias.baddbmm_(q, k, scale) so we have to use beta_one
  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            k_seq_len,
            q_seq_len,
            head_dim,
            &scale, /* host pointer */
            static_cast<const void*>(k_lin_results_ptr),
            lead_dim,
            batch_stride,
            static_cast<const void*>(q_lin_results_ptr),
            lead_dim,
            batch_stride,
            &beta_one, /* host pointer */
            static_cast<void*>(attn_scores_ptr), // C
            k_seq_len, // ldc
            k_seq_len*q_seq_len, // stride c
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM QK forward failed with %d\n", cublas_status);
      exit(0);
  }

  if (use_time_mask){
    attn_scores.masked_fill_(pad_mask, -std::numeric_limits<float>::infinity());
  }

  // bias is already a part of

//   else {
//     attn_scores.view({sequences, heads, q_seq_len, k_seq_len}).masked_fill_(pad_mask,
//                                                                           -std::numeric_limits<float>::infinity());
//   }
  // Padded Softmax
  bool softmax_success = false;


  // here we should prepare the masked bias: so that the padded positions have -inf values
  if (is_training && dropout_prob > 0.0f) {
      // This function fuses softmax-dropout-pad (and dropout inplace)
      softmax_success = dispatch_softmax_dropout<half, half, float>(
                           reinterpret_cast<half*>(dropout_results_ptr),
                           (is_training) ? reinterpret_cast<uint8_t*>(dropout_mask.data_ptr<uint8_t>()) : nullptr,
                           reinterpret_cast<const half*>(attn_scores_ptr),
      		               dropout_elems,
                           k_seq_len,
                           k_seq_len,
                           attn_batches*q_seq_len,
      		               1.0f-dropout_prob,
		                   stream);
  } else {
      softmax_success = dispatch_softmax<half, half, float>(
                             reinterpret_cast<half*>(dropout_results_ptr), // this is actually softmax results, but making it consistent for the next function
                             reinterpret_cast<const half*>(attn_scores_ptr),
                             k_seq_len,
                             k_seq_len,
                             attn_batches*q_seq_len,
                             stream);  // pad batch strides
  }

  assert(softmax_success);

  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            head_dim,
            q_seq_len,
            k_seq_len,
            &alpha, /* host pointer */
            static_cast<const void*>(v_lin_results_ptr),  // A:
            lead_dim,
            batch_stride,
            static_cast<const void*>(dropout_results.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta_zero, /* host pointer */
            static_cast<void*>(matmul2_results.data_ptr()), // C
            head_dim*attn_batches,
            head_dim,
            attn_batches,
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM Context forward failed with %d\n", cublas_status);
      exit(0);
  }

  cublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            embed_dim,
            batches,
            embed_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(output_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(matmul2_results.data_ptr()),
            embed_dim,
            &beta_zero, /* host pointer */
            static_cast<void*>(outputs.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            true,
            static_cast<const void*>(output_biases.data_ptr()));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM output forward failed with %d\n", cublas_status);
      exit(0);
  }
  TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));

  return {
           input_lin_results,
           attn_scores,
           dropout_results,
           dropout_mask,
           matmul2_results,
           outputs
         };
}



std::vector<torch::Tensor> bwd_cuda(
                               bool use_time_mask,
                               int                  heads,
                               torch::Tensor const& output_grads,
                               torch::Tensor const& matmul2_results,
                               torch::Tensor const& dropout_results,
                               torch::Tensor const& attn_scores,
                               torch::Tensor const& input_lin_results,
                               torch::Tensor const& inputs,
                               torch::Tensor const& input_weights,
                               torch::Tensor const& output_weights,
                               torch::Tensor const& dropout_mask,
                               float                dropout_prob,
                               torch::Tensor lt_workspace)

{
  int   embed_dim      = inputs.size(2);
  int   sequences      = inputs.size(1);
  int   q_seq_len      = inputs.size(0);
  int   k_seq_len      = q_seq_len;
  int   batches        = sequences * q_seq_len;
  int   head_dim       = embed_dim / heads;
  int   output_lin_dim = 3 * embed_dim;
  int   attn_batches   = heads * sequences;
  int   lead_dim       = attn_batches * 3 * head_dim;
  int   batch_stride   = 3 * head_dim;
//   const int   dropout_elems  = attn_batches * q_seq_len * k_seq_len;
  const float alpha          = 1.0;
  const float beta           = 0.0;
  const float scale          = 1.0 / sqrt(static_cast<float>(head_dim));
  const half halpha = __float2half_rn(alpha);
  const half hbeta = __float2half_rn(beta);
  const half hscale = __float2half_rn(scale);

  // TODO: Streams can be used in Backprop but I haven't added more than one
  // in my first attempt to create the code
  cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();
  cudaStream_t   stream = at::cuda::getCurrentCUDAStream().stream();
  cublasSetStream(handle, stream);

  // Output Tensor Allocations
  torch::Tensor input_grads         = torch::empty_like(inputs);
  torch::Tensor input_weight_grads  = torch::empty_like(input_weights);
  torch::Tensor output_weight_grads = torch::empty_like(output_weights);
  // Intermediate Tensor Allocations
  at::Tensor output_lin_grads       = torch::empty_like(matmul2_results);
  at::Tensor matmul2_grads          = torch::empty_like(dropout_results);
  at::Tensor input_lin_output_grads = torch::empty_like(input_lin_results);

  at::Tensor input_biases_grads = torch::empty({output_lin_dim}, inputs.options());
  at::Tensor output_biases_grads = torch::empty({embed_dim}, inputs.options());

  auto q_lin_results_ptr = static_cast<half*>(input_lin_results.data_ptr());
  auto k_lin_results_ptr = static_cast<half*>(input_lin_results.data_ptr()) + head_dim;
  auto v_lin_results_ptr = static_cast<half*>(input_lin_results.data_ptr()) + 2*head_dim;

//   auto q_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr());
  auto q_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr());
  auto k_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr()) + head_dim;
  auto v_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr()) + 2*head_dim;

  void* lt_workspace_ptr = static_cast<void*>(lt_workspace.data_ptr());

  TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));


  int cublas_status = 1;

  // Output Linear Dgrad
  cublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            embed_dim,
            batches,
            embed_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(output_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(output_grads.data_ptr()),
            embed_dim,
            &beta, /* host pointer */
            static_cast<void*>(output_lin_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            false,
            static_cast<const void*>(nullptr));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM output lin grad backward failed with %d\n", cublas_status);
      exit(0);
  }

  cublas_status = gemm_bgradb_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            embed_dim,
            embed_dim,
            batches,
            &alpha, /* host pointer */
            static_cast<const void*>(matmul2_results.data_ptr()),
            embed_dim,
            static_cast<const void*>(output_grads.data_ptr()),
            embed_dim,
            &beta, /* host pointer */
            static_cast<void*>(output_weight_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr,
            1 << 22,
            stream,
            true,
            static_cast<void*>(output_biases_grads.data_ptr()));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM output backward failed with %d\n", cublas_status);
      exit(0);
  }

  // MatMul2 Dgrad1

  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            k_seq_len,
            q_seq_len,
            head_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(v_lin_results_ptr),  // A:
            lead_dim,
            batch_stride,
            static_cast<const void*>(output_lin_grads.data_ptr()),
            head_dim*attn_batches,
            head_dim,
            &beta, /* host pointer */
            static_cast<void*>(matmul2_grads.data_ptr()), // C
            k_seq_len,
            k_seq_len*q_seq_len,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM backward 1 failed with %d\n", cublas_status);
      exit(0);
  }

  // Matmul2 Dgrad2

  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            head_dim,
            k_seq_len,
            q_seq_len,
            &alpha, /* host pointer */
            static_cast<const void*>(output_lin_grads.data_ptr()),  // A:
            head_dim*attn_batches,
            head_dim,
            static_cast<const void*>(dropout_results.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta, /* host pointer */
            static_cast<void*>(v_lin_grads_ptr), // C
            lead_dim,
            batch_stride,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM backward 2 failed with %d\n", cublas_status);
      exit(0);
  }
  // Apply Dropout Mask and Scale by Dropout Probability
  // Softmax Grad

  if ( dropout_prob > 0.0f) {
      dispatch_softmax_dropout_backward_recompute<half, half, float, false>(
                                     static_cast<half*>(matmul2_grads.data_ptr()),
                                     static_cast<const half*>(matmul2_grads.data_ptr()),
                                     reinterpret_cast<const half*>(attn_scores.data_ptr()), // need this to recompute softmax
                                     //reinterpret_cast<half const*>(pad_mask.data_ptr()),
                                     static_cast<uint8_t const*>(dropout_mask.data_ptr()),
                                     1.0/(1.0-dropout_prob),
                                     k_seq_len,
                                     k_seq_len,
                                     attn_batches*q_seq_len,
                                     stream);
  } else {
//       if dropout == 0 then we don't need to recompute (because dropout_results == softmax_results)
      dispatch_softmax_backward_norecompute<half, half, float, false>(
                                 static_cast<half*>(matmul2_grads.data_ptr()),
                                 static_cast<const half*>(matmul2_grads.data_ptr()),
                                 reinterpret_cast<const half*>(dropout_results.data_ptr()),
                                 k_seq_len,
                                 k_seq_len,
//                                  attn_batches*q_seq_len/sequences,
                                 attn_batches*q_seq_len,
                                 stream);
  }

  // Matmul1 Dgrad1
  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            head_dim,
            q_seq_len,
            k_seq_len,
            &scale, /* host pointer */
            static_cast<const void*>(k_lin_results_ptr),  // A:
            lead_dim,
            batch_stride,
            static_cast<const void*>(matmul2_grads.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta, /* host pointer */
            static_cast<void*>(q_lin_grads_ptr), // C
            lead_dim,
            batch_stride,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM backward 1 failed with %d\n", cublas_status);
      exit(0);
  }

  // Matmul1 Dgrad2
  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            head_dim,
            k_seq_len,
            q_seq_len,
            &scale, /* host pointer */
            static_cast<const void*>(q_lin_results_ptr),  // A:
            lead_dim,
            batch_stride,
            static_cast<const void*>(matmul2_grads.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta, /* host pointer */
            static_cast<void*>(k_lin_grads_ptr), // C
            lead_dim,
            batch_stride,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM backward 2 failed with %d\n", cublas_status);
      exit(0);
  }

  // Input Linear Dgrad
  cublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            embed_dim,
            batches,
            output_lin_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(input_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(input_lin_output_grads.data_ptr()),
            output_lin_dim,
            &beta, /* host pointer */
            static_cast<void*>(input_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            false,
            static_cast<const void*>(nullptr));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM output backward final input failed with %d\n", cublas_status);
      exit(0);
  }

  // Input Linear Wgrad
  cublas_status = gemm_bgradb_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            embed_dim,
            output_lin_dim,
            batches,
            &alpha, /* host pointer */
            static_cast<const void*>(inputs.data_ptr()),
            embed_dim,
            reinterpret_cast<const void*>(q_lin_grads_ptr),
            output_lin_dim,
            &beta, /* host pointer */
            static_cast<void*>(input_weight_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            true,
            static_cast<void*>(input_biases_grads.data_ptr()));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM input backward failed with %d\n", cublas_status);
      exit(0);
  }

  TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));

  return {
           input_grads,
           input_weight_grads,
           output_weight_grads,
           input_biases_grads,
           output_biases_grads
         };
}

std::vector<torch::Tensor> bwd_bias_cuda(
                               bool use_time_mask,
                               int                  heads,
                               torch::Tensor const& output_grads,
                               torch::Tensor const& matmul2_results,
                               torch::Tensor const& dropout_results,
                               torch::Tensor const& attn_scores,
                               torch::Tensor const& input_lin_results,
                               torch::Tensor const& inputs,
                               torch::Tensor const& input_weights,
                               torch::Tensor const& output_weights,
                               torch::Tensor const& dropout_mask,
                               float                dropout_prob,
                               torch::Tensor lt_workspace)

{
  int   embed_dim      = inputs.size(2);
  int   sequences      = inputs.size(1);
  int   q_seq_len      = inputs.size(0);
  int   k_seq_len      = q_seq_len;
  int   batches        = sequences * q_seq_len;
  int   head_dim       = embed_dim / heads;
  int   output_lin_dim = 3 * embed_dim;
  int   attn_batches   = heads * sequences;
  int   lead_dim       = attn_batches * 3 * head_dim;
  int   batch_stride   = 3 * head_dim;
//   const int   dropout_elems  = attn_batches * q_seq_len * k_seq_len;
  const float alpha          = 1.0;
  const float beta           = 0.0;
  const float scale          = 1.0 / sqrt(static_cast<float>(head_dim));
  const half halpha = __float2half_rn(alpha);
  const half hbeta = __float2half_rn(beta);
  const half hscale = __float2half_rn(scale);

  // TODO: Streams can be used in Backprop but I haven't added more than one
  // in my first attempt to create the code
  cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();
  cudaStream_t   stream = at::cuda::getCurrentCUDAStream().stream();
  cublasSetStream(handle, stream);

  // Output Tensor Allocations
  torch::Tensor input_grads         = torch::empty_like(inputs);
  torch::Tensor input_weight_grads  = torch::empty_like(input_weights);
  torch::Tensor output_weight_grads = torch::empty_like(output_weights);
  // Intermediate Tensor Allocations
  at::Tensor output_lin_grads       = torch::empty_like(matmul2_results);
  at::Tensor matmul2_grads          = torch::empty_like(dropout_results);
  at::Tensor input_lin_output_grads = torch::empty_like(input_lin_results);

  at::Tensor input_biases_grads = torch::empty({output_lin_dim}, inputs.options());
  at::Tensor output_biases_grads = torch::empty({embed_dim}, inputs.options());

  auto q_lin_results_ptr = static_cast<half*>(input_lin_results.data_ptr());
  auto k_lin_results_ptr = static_cast<half*>(input_lin_results.data_ptr()) + head_dim;
  auto v_lin_results_ptr = static_cast<half*>(input_lin_results.data_ptr()) + 2*head_dim;

//   auto q_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr());
  auto q_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr());
  auto k_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr()) + head_dim;
  auto v_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr()) + 2*head_dim;

  void* lt_workspace_ptr = static_cast<void*>(lt_workspace.data_ptr());

  TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));


  int cublas_status = 1;

  // Output Linear Dgrad
  cublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            embed_dim,
            batches,
            embed_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(output_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(output_grads.data_ptr()),
            embed_dim,
            &beta, /* host pointer */
            static_cast<void*>(output_lin_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            false,
            static_cast<const void*>(nullptr));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM output lin grad backward failed with %d\n", cublas_status);
      exit(0);
  }

  cublas_status = gemm_bgradb_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            embed_dim,
            embed_dim,
            batches,
            &alpha, /* host pointer */
            static_cast<const void*>(matmul2_results.data_ptr()),
            embed_dim,
            static_cast<const void*>(output_grads.data_ptr()),
            embed_dim,
            &beta, /* host pointer */
            static_cast<void*>(output_weight_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr,
            1 << 22,
            stream,
            true,
            static_cast<void*>(output_biases_grads.data_ptr()));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM output backward failed with %d\n", cublas_status);
      exit(0);
  }

  // MatMul2 Dgrad1

  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            k_seq_len,
            q_seq_len,
            head_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(v_lin_results_ptr),  // A:
            lead_dim,
            batch_stride,
            static_cast<const void*>(output_lin_grads.data_ptr()),
            head_dim*attn_batches,
            head_dim,
            &beta, /* host pointer */
            static_cast<void*>(matmul2_grads.data_ptr()), // C
            k_seq_len,
            k_seq_len*q_seq_len,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM backward 1 failed with %d\n", cublas_status);
      exit(0);
  }

  // Matmul2 Dgrad2

  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            head_dim,
            k_seq_len,
            q_seq_len,
            &alpha, /* host pointer */
            static_cast<const void*>(output_lin_grads.data_ptr()),  // A:
            head_dim*attn_batches,
            head_dim,
            static_cast<const void*>(dropout_results.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta, /* host pointer */
            static_cast<void*>(v_lin_grads_ptr), // C
            lead_dim,
            batch_stride,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM backward 2 failed with %d\n", cublas_status);
      exit(0);
  }
  // Apply Dropout Mask and Scale by Dropout Probability
  // Softmax Grad

  if ( dropout_prob > 0.0f) {
      // we don't need pad_mask because the stored attn_scores already is padded
      dispatch_softmax_dropout_backward_recompute<half, half, float, false>(
                                     static_cast<half*>(matmul2_grads.data_ptr()),
                                     static_cast<const half*>(matmul2_grads.data_ptr()),
                                     reinterpret_cast<const half*>(attn_scores.data_ptr()), // need this to recompute softmax
                                     //reinterpret_cast<half const*>(pad_mask.data_ptr()),
                                     static_cast<uint8_t const*>(dropout_mask.data_ptr()),
                                     1.0/(1.0-dropout_prob),
                                     k_seq_len,
                                     k_seq_len,
                                     attn_batches*q_seq_len,
                                     stream);
  } else {
//       if dropout == 0 then we don't need to recompute (because dropout_results == softmax_results)
      dispatch_softmax_backward_norecompute<half, half, float, false>(
                                 static_cast<half*>(matmul2_grads.data_ptr()),
                                 static_cast<const half*>(matmul2_grads.data_ptr()),
                                 reinterpret_cast<const half*>(dropout_results.data_ptr()),
                                 k_seq_len,
                                 k_seq_len,
//                                  attn_batches*q_seq_len/sequences,
                                 attn_batches*q_seq_len,
                                 stream);
  }

  // Matmul1 Dgrad1
  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            head_dim,
            q_seq_len,
            k_seq_len,
            &scale, /* host pointer */
            static_cast<const void*>(k_lin_results_ptr),  // A:
            lead_dim,
            batch_stride,
            static_cast<const void*>(matmul2_grads.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta, /* host pointer */
            static_cast<void*>(q_lin_grads_ptr), // C
            lead_dim,
            batch_stride,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM backward 1 failed with %d\n", cublas_status);
      exit(0);
  }

  // Matmul1 Dgrad2
  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            head_dim,
            k_seq_len,
            q_seq_len,
            &scale, /* host pointer */
            static_cast<const void*>(q_lin_results_ptr),  // A:
            lead_dim,
            batch_stride,
            static_cast<const void*>(matmul2_grads.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta, /* host pointer */
            static_cast<void*>(k_lin_grads_ptr), // C
            lead_dim,
            batch_stride,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM backward 2 failed with %d\n", cublas_status);
      exit(0);
  }

  // Input Linear Dgrad
  cublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            embed_dim,
            batches,
            output_lin_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(input_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(input_lin_output_grads.data_ptr()),
            output_lin_dim,
            &beta, /* host pointer */
            static_cast<void*>(input_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            false,
            static_cast<const void*>(nullptr));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM output backward final input failed with %d\n", cublas_status);
      exit(0);
  }

  // Input Linear Wgrad
  cublas_status = gemm_bgradb_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            embed_dim,
            output_lin_dim,
            batches,
            &alpha, /* host pointer */
            static_cast<const void*>(inputs.data_ptr()),
            embed_dim,
            reinterpret_cast<const void*>(q_lin_grads_ptr),
            output_lin_dim,
            &beta, /* host pointer */
            static_cast<void*>(input_weight_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            true,
            static_cast<void*>(input_biases_grads.data_ptr()));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM input backward failed with %d\n", cublas_status);
      exit(0);
  }

  TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));
  at::Tensor bias_grads =  matmul2_grads;

  return {
           input_grads,
           input_weight_grads,
           output_weight_grads,
           input_biases_grads,
           output_biases_grads,
           bias_grads
         };
}


std::vector<torch::Tensor> bwd_cuda_recompute(
                               bool use_time_mask,
                               int                  heads,
                               torch::Tensor const& output_grads,
                               torch::Tensor const& inputs,
                               torch::Tensor const& input_weights,
                               torch::Tensor const& output_weights,
                               torch::Tensor const& input_biases,
                               torch::Tensor const& output_biases,
                               torch::Tensor const& pad_mask,
                               torch::Tensor const& dropout_mask,
                               float                dropout_prob,
                               torch::Tensor lt_workspace)

{
  int   embed_dim      = inputs.size(2);
  int   sequences      = inputs.size(1);
  int   q_seq_len      = inputs.size(0);
  int   k_seq_len      = q_seq_len;
  int   batches        = sequences * q_seq_len;
  int   head_dim       = embed_dim / heads;
  int   output_lin_dim = 3 * embed_dim;
  int   attn_batches   = heads * sequences;
  int   lead_dim       = attn_batches * 3 * head_dim;
  int   batch_stride   = 3 * head_dim;
  const int   dropout_elems  = attn_batches * q_seq_len * k_seq_len;
  const float alpha          = 1.0;
  const float beta           = 0.0;
  const float beta_zero           = 0.0;
  const float scale          = 1.0 / sqrt(static_cast<float>(head_dim));
  const half halpha = __float2half_rn(alpha);
  const half hbeta = __float2half_rn(beta);
  const half hscale = __float2half_rn(scale);

  // TODO: Streams can be used in Backprop but I haven't added more than one
  // in my first attempt to create the code
  cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();
  cudaStream_t   stream = at::cuda::getCurrentCUDAStream().stream();
  cublasSetStream(handle, stream);

  //////////////////////////////////// RECOMPUTE ACTIVATIONS ////////////////////////////////////////

  // 3 Intermediate Results + Output (Note: dropout intermediates are generated by ATen library code)
  auto act_options  = inputs.options().requires_grad(false);
  auto mask_options = act_options.dtype(torch::kUInt8);

  torch::Tensor input_lin_results = torch::empty({q_seq_len, sequences, output_lin_dim}, act_options);
  torch::Tensor attn_scores       = torch::empty({attn_batches, q_seq_len, k_seq_len},      act_options);
  torch::Tensor dropout_results   = torch::empty({attn_batches, q_seq_len, k_seq_len},   act_options);
  torch::Tensor matmul2_results   = torch::empty({q_seq_len, attn_batches, head_dim},    act_options);

  // Input Linear Results Pointers to Q, K, and V of interviewed activations
  void* q_lin_results_ptr   = static_cast<void*>(input_lin_results.data_ptr());
  void* k_lin_results_ptr   = static_cast<void*>(static_cast<half*>(input_lin_results.data_ptr()) + head_dim);
  void* v_lin_results_ptr   = static_cast<void*>(static_cast<half*>(input_lin_results.data_ptr()) + 2*head_dim);

  // Softmax Intermediate Result Ptr (used by Matmul1 -> Softmax)
  void* attn_scores_ptr = static_cast<void*>(attn_scores.data_ptr());
  void* dropout_results_ptr = static_cast<void*>(dropout_results.data_ptr());

  void* lt_workspace_ptr = static_cast<void*>(lt_workspace.data_ptr());

  TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));

  int rcublas_status = 1;
  rcublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            output_lin_dim,
            batches,
            embed_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(input_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(inputs.data_ptr()),
            embed_dim,
            &beta_zero, /* host pointer */
            q_lin_results_ptr,
            output_lin_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            true,
            static_cast<const void*>(input_biases.data_ptr()));

  if (rcublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM QKV recompute failed with %d\n", rcublas_status);
      exit(0);
  }

  rcublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            k_seq_len,
            q_seq_len,
            head_dim,
            &scale, /* host pointer */
            static_cast<const void*>(k_lin_results_ptr),
            lead_dim,
            batch_stride,
            static_cast<const void*>(q_lin_results_ptr),
            lead_dim,
            batch_stride,
            &beta_zero, /* host pointer */
            static_cast<void*>(attn_scores_ptr), // C
            k_seq_len, // ldc
            k_seq_len*q_seq_len, // stride c
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (rcublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM QKV recompute forward failed with %d\n", rcublas_status);
      exit(0);
  }

  if (use_time_mask){
    attn_scores.masked_fill_(pad_mask, -std::numeric_limits<float>::infinity());
  } else {
    attn_scores.view({sequences, heads, q_seq_len, k_seq_len}).masked_fill_(pad_mask,
                                                                          -std::numeric_limits<float>::infinity());
  }
  // Padded Softmax

  bool softmax_forward_success = false;
  if (dropout_prob > 0.0f) {
      // This function fuses softmax-dropout-pad (and dropout inplace)
      softmax_forward_success = dispatch_softmax_dropout_presampled<half, half, float>(
                           reinterpret_cast<half*>(dropout_results_ptr),
                           reinterpret_cast<const uint8_t*>(dropout_mask.data_ptr<uint8_t>()),
                           reinterpret_cast<const half*>(attn_scores_ptr),
      		               dropout_elems,
                           k_seq_len,
                           k_seq_len,
                           attn_batches*q_seq_len,
      		               1.0f-dropout_prob,
		                   stream);
  } else {
      softmax_forward_success = dispatch_softmax<half, half, float>(
                             reinterpret_cast<half*>(dropout_results_ptr), // this is actually softmax results, but making it consistent for the next function
                             reinterpret_cast<const half*>(attn_scores_ptr),
                             k_seq_len,
                             k_seq_len,
                             attn_batches*q_seq_len,
                             stream);  // pad batch strides
  }

  assert(softmax_forward_success);

  rcublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            head_dim,
            q_seq_len,
            k_seq_len,
            &alpha, /* host pointer */
            static_cast<const void*>(v_lin_results_ptr),  // A:
            lead_dim,
            batch_stride,
            static_cast<const void*>(dropout_results.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta_zero, /* host pointer */
            static_cast<void*>(matmul2_results.data_ptr()), // C
            head_dim*attn_batches,
            head_dim,
            attn_batches,
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (rcublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM Context forward failed with %d\n", rcublas_status);
      exit(0);
  }

  ////////////////////////////////////// BACKWARD PASS STARTS ////////////////////////////////////////

  // Output Tensor Allocations
  torch::Tensor input_grads         = torch::empty_like(inputs);
  torch::Tensor input_weight_grads  = torch::empty_like(input_weights);
  torch::Tensor output_weight_grads = torch::empty_like(output_weights);
  // Intermediate Tensor Allocations
  at::Tensor output_lin_grads       = torch::empty_like(matmul2_results);
  at::Tensor matmul2_grads          = torch::empty_like(dropout_results);
  at::Tensor input_lin_output_grads = torch::empty_like(input_lin_results);

  at::Tensor input_biases_grads = torch::empty({output_lin_dim}, inputs.options());
  at::Tensor output_biases_grads = torch::empty({embed_dim}, inputs.options());

  auto q_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr());
  auto k_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr()) + head_dim;
  auto v_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr()) + 2*head_dim;

//   void* lt_workspace_ptr = static_cast<void*>(lt_workspace.data_ptr());

  TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));

  int cublas_status = 1;

  // Output Linear Dgrad
  cublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            embed_dim,
            batches,
            embed_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(output_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(output_grads.data_ptr()),
            embed_dim,
            &beta, /* host pointer */
            static_cast<void*>(output_lin_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            false,
            static_cast<const void*>(nullptr));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM output lin grad backward failed with %d\n", cublas_status);
      exit(0);
  }

  cublas_status = gemm_bgradb_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            embed_dim,
            embed_dim,
            batches,
            &alpha, /* host pointer */
            static_cast<const void*>(matmul2_results.data_ptr()),
            embed_dim,
            static_cast<const void*>(output_grads.data_ptr()),
            embed_dim,
            &beta, /* host pointer */
            static_cast<void*>(output_weight_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr,
            1 << 22,
            stream,
            true,
            static_cast<void*>(output_biases_grads.data_ptr()));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM output backward failed with %d\n", cublas_status);
      exit(0);
  }

  // MatMul2 Dgrad1

  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            k_seq_len,
            q_seq_len,
            head_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(v_lin_results_ptr),  // A:
            lead_dim,
            batch_stride,
            static_cast<const void*>(output_lin_grads.data_ptr()),
            head_dim*attn_batches,
            head_dim,
            &beta, /* host pointer */
            static_cast<void*>(matmul2_grads.data_ptr()), // C
            k_seq_len,
            k_seq_len*q_seq_len,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM backward 1 failed with %d\n", cublas_status);
      exit(0);
  }

  // Matmul2 Dgrad2

  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            head_dim,
            k_seq_len,
            q_seq_len,
            &alpha, /* host pointer */
            static_cast<const void*>(output_lin_grads.data_ptr()),  // A:
            head_dim*attn_batches,
            head_dim,
            static_cast<const void*>(dropout_results.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta, /* host pointer */
            static_cast<void*>(v_lin_grads_ptr), // C
            lead_dim,
            batch_stride,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM backward 2 failed with %d\n", cublas_status);
      exit(0);
  }
  // Apply Dropout Mask and Scale by Dropout Probability
  // Softmax Grad

  if ( dropout_prob > 0.0f) {
      dispatch_softmax_dropout_backward_recompute<half, half, float, false>(
                                     static_cast<half*>(matmul2_grads.data_ptr()),
                                     static_cast<const half*>(matmul2_grads.data_ptr()),
                                     reinterpret_cast<const half*>(attn_scores.data_ptr()), // need this to recompute softmax
                                     //reinterpret_cast<half const*>(pad_mask.data_ptr()),
                                     static_cast<uint8_t const*>(dropout_mask.data_ptr()),
                                     1.0/(1.0-dropout_prob),
                                     k_seq_len,
                                     k_seq_len,
                                     attn_batches*q_seq_len,
                                     stream);
  } else {
//       if dropout == 0 then we don't need to recompute (because dropout_results == softmax_results)
      dispatch_softmax_backward_norecompute<half, half, float, false>(
                                 static_cast<half*>(matmul2_grads.data_ptr()),
                                 static_cast<const half*>(matmul2_grads.data_ptr()),
                                 reinterpret_cast<const half*>(dropout_results.data_ptr()),
                                 k_seq_len,
                                 k_seq_len,
//                                  attn_batches*q_seq_len/sequences,
                                 attn_batches*q_seq_len,
                                 stream);
  }

  // Matmul1 Dgrad1
  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            head_dim,
            q_seq_len,
            k_seq_len,
            &scale, /* host pointer */
            static_cast<const void*>(k_lin_results_ptr),  // A:
            lead_dim,
            batch_stride,
            static_cast<const void*>(matmul2_grads.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta, /* host pointer */
            static_cast<void*>(q_lin_grads_ptr), // C
            lead_dim,
            batch_stride,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM backward 1 failed with %d\n", cublas_status);
      exit(0);
  }

  // Matmul1 Dgrad2
  cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            head_dim,
            k_seq_len,
            q_seq_len,
            &scale, /* host pointer */
            static_cast<const void*>(q_lin_results_ptr),  // A:
            lead_dim,
            batch_stride,
            static_cast<const void*>(matmul2_grads.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta, /* host pointer */
            static_cast<void*>(k_lin_grads_ptr), // C
            lead_dim,
            batch_stride,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM backward 2 failed with %d\n", cublas_status);
      exit(0);
  }

  // Input Linear Dgrad
  cublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            embed_dim,
            batches,
            output_lin_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(input_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(input_lin_output_grads.data_ptr()),
            output_lin_dim,
            &beta, /* host pointer */
            static_cast<void*>(input_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            false,
            static_cast<const void*>(nullptr));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM output backward final input failed with %d\n", cublas_status);
      exit(0);
  }

  // Input Linear Wgrad
  cublas_status = gemm_bgradb_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            embed_dim,
            output_lin_dim,
            batches,
            &alpha, /* host pointer */
            static_cast<const void*>(inputs.data_ptr()),
            embed_dim,
            reinterpret_cast<const void*>(q_lin_grads_ptr),
            output_lin_dim,
            &beta, /* host pointer */
            static_cast<void*>(input_weight_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            true,
            static_cast<void*>(input_biases_grads.data_ptr()));

  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM input backward failed with %d\n", cublas_status);
      exit(0);
  }

  TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));

  return {
           input_grads,
           input_weight_grads,
           output_weight_grads,
           input_biases_grads,
           output_biases_grads
         };
}




//
// torch::Tensor bwd_cuda_input_only(
//                                bool use_time_mask,
//                                int                  heads,
//                                torch::Tensor const& output_grads,
//                                torch::Tensor const& matmul2_results,
//                                torch::Tensor const& dropout_results,
//                                torch::Tensor const& attn_scores,
//                                torch::Tensor const& input_lin_results,
//                                torch::Tensor const& inputs,
//                                torch::Tensor const& input_weights,
//                                torch::Tensor const& output_weights,
//                                torch::Tensor const& dropout_mask,
//                                float                dropout_prob
//                                    )
// {
//   const int   embed_dim      = inputs.size(2);
//   const int   sequences      = inputs.size(1);
//   const int   q_seq_len      = inputs.size(0);
//   const int   k_seq_len      = q_seq_len;
//   const int   batches        = sequences * q_seq_len;
//   const int   head_dim       = embed_dim / heads;
//   const int   output_lin_dim = 3 * embed_dim;
//   const int   attn_batches   = heads * sequences;
//   const int   lead_dim       = attn_batches * 3 * head_dim;
//   const int   batch_stride   = 3 * head_dim;
// //   const int   dropout_elems  = attn_batches * q_seq_len * k_seq_len;
//   const float alpha          = 1.0;
//   const float beta           = 0.0;
//   const float scale          = 1.0 / sqrt(static_cast<float>(head_dim));
//   const half halpha = __float2half_rn(alpha);
//   const half hbeta = __float2half_rn(beta);
//   const half hscale = __float2half_rn(scale);
//
//   // TODO: Streams can be used in Backprop but I haven't added more than one
//   // in my first attempt to create the code
//   cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();
//   cudaStream_t   stream = at::cuda::getCurrentCUDAStream().stream();
//   cublasSetStream(handle, stream);
//
//   // Output Tensor Allocations
//   torch::Tensor input_grads         = torch::empty_like(inputs);
// //  torch::Tensor input_weight_grads  = torch::empty_like(input_weights);
// //  torch::Tensor output_weight_grads = torch::empty_like(output_weights);
//   // Intermediate Tensor Allocations
//   at::Tensor output_lin_grads       = torch::empty_like(matmul2_results);
//   at::Tensor matmul2_grads          = torch::empty_like(dropout_results);
//   at::Tensor input_lin_output_grads = torch::empty_like(input_lin_results);
//
//   auto q_lin_results_ptr = static_cast<half*>(input_lin_results.data_ptr());
//   auto k_lin_results_ptr = static_cast<half*>(input_lin_results.data_ptr()) + head_dim;
//   auto v_lin_results_ptr = static_cast<half*>(input_lin_results.data_ptr()) + 2*head_dim;
//
//   auto q_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr());
//   auto k_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr()) + head_dim;
//   auto v_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr()) + 2*head_dim;
//
// //  char a_layout_n{'n'};
// //  char a_layout_t{'t'};
// //  char b_layout_n{'n'};
// //  char b_layout_t{'t'};
//
//   TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));
//
//   // Output Linear Dgrad
//   TORCH_CUDABLAS_CHECK(cublasGemmEx(handle,
//                              CUBLAS_OP_N,
//                              CUBLAS_OP_N,
//                              embed_dim,
//                              batches,
//                              embed_dim,
//                              static_cast<const void*>(&alpha),
//                              static_cast<const void*>(output_weights.data_ptr()),
//                              CUDA_R_16F,
//                              embed_dim,
//                              static_cast<const void*>(output_grads.data_ptr()),
//                              CUDA_R_16F,
//                              embed_dim,
//                              static_cast<const void*>(&beta),
//                              static_cast<void*>(output_lin_grads.data_ptr()),
//                              CUDA_R_16F,
//                              embed_dim,
//                              CUDA_R_32F,
//                              CUBLAS_GEMM_DEFAULT_TENSOR_OP));
//   // Output Linear Wgrad
// //   TORCH_CUDABLAS_CHECK(cublasGemmEx(handle,
// //                              CUBLAS_OP_N,
// //                              CUBLAS_OP_T,
// //                              embed_dim,
// //                              embed_dim,
// //                              batches,
// //                              static_cast<const void*>(&alpha),
// //                              static_cast<const void*>(matmul2_results.data_ptr()),
// //                              CUDA_R_16F,
// //                              embed_dim,
// //                              static_cast<const void*>(output_grads.data_ptr()),
// //                              CUDA_R_16F,
// //                              embed_dim,
// //                              static_cast<const void*>(&beta),
// //                              static_cast<void*>(output_weight_grads.data_ptr()),
// //                              CUDA_R_16F,
// //                              embed_dim,
// //                              CUDA_R_32F,
// //                              CUBLAS_GEMM_DEFAULT_TENSOR_OP));
// //
// //   auto  output_bias_grads = output_grads.view({-1, embed_dim}).sum(0, false);
//   // MatMul2 Dgrad1
//
//   TORCH_CUDABLAS_CHECK(cublasGemmStridedBatchedEx(handle,
//                              CUBLAS_OP_T,
//                              CUBLAS_OP_N,
//                              k_seq_len,    // m
//                              q_seq_len,   // n
//                              head_dim,   // k
//                              static_cast<const void*>(&alpha),
//                              static_cast<const void*>(v_lin_results_ptr),  // A:
//                              CUDA_R_16F,
//                              lead_dim,  // lda
//                              batch_stride, // stride A
//                              static_cast<const void*>(output_lin_grads.data_ptr()),
//                              CUDA_R_16F,
//                              head_dim*attn_batches,
//                              head_dim,
//                              static_cast<const void*>(&beta),
//                              static_cast<void*>(matmul2_grads.data_ptr()), // C
//                              CUDA_R_16F,
//                              k_seq_len,
//                              k_seq_len*q_seq_len,
//                              attn_batches,
//                              CUDA_R_32F,
//                              CUBLAS_GEMM_DEFAULT_TENSOR_OP));
//
//   // Matmul2 Dgrad2
//   TORCH_CUDABLAS_CHECK(cublasGemmStridedBatchedEx(handle,
//                              CUBLAS_OP_N,
//                              CUBLAS_OP_T,
//                              head_dim,    // m
//                              k_seq_len,   // n
//                              q_seq_len,   // k
//                              static_cast<const void*>(&alpha),
//                              static_cast<const void*>(output_lin_grads.data_ptr()),  // A:
//                              CUDA_R_16F,
//                              head_dim*attn_batches,  // lda
//                              head_dim, // stride A
//                              static_cast<const void*>(dropout_results.data_ptr()),
//                              CUDA_R_16F,
//                              k_seq_len,
//                              k_seq_len*q_seq_len,
//                              static_cast<const void*>(&beta),
//                              static_cast<void*>(v_lin_grads_ptr), // C
//                              CUDA_R_16F,
//                              lead_dim,
//                              batch_stride,
//                              attn_batches,
//                              CUDA_R_32F,
//                              CUBLAS_GEMM_DEFAULT_TENSOR_OP));
//
//   // Apply Dropout Mask and Scale by Dropout Probability
//   // Softmax Grad
//
//   if ( dropout_prob > 0.0f) {
//       dispatch_softmax_dropout_backward_recompute<half, half, float, false>(
//                                      static_cast<half*>(matmul2_grads.data_ptr()),
//                                      static_cast<const half*>(matmul2_grads.data_ptr()),
//                                      reinterpret_cast<const half*>(attn_scores.data_ptr()), // need this to recompute softmax
//                                      //reinterpret_cast<half const*>(pad_mask.data_ptr()),
//                                      static_cast<uint8_t const*>(dropout_mask.data_ptr()),
//                                      1.0/(1.0-dropout_prob),
//                                      k_seq_len,
//                                      k_seq_len,
//                                      attn_batches*q_seq_len,
//                                      stream);
//   } else {
// //       if dropout == 0 then we don't need to recompute (because dropout_results == softmax_results)
//       dispatch_softmax_backward_norecompute<half, half, float, false>(
//                                  static_cast<half*>(matmul2_grads.data_ptr()),
//                                  static_cast<const half*>(matmul2_grads.data_ptr()),
//                                  reinterpret_cast<const half*>(dropout_results.data_ptr()),
//                                  k_seq_len,
//                                  k_seq_len,
// //                                  attn_batches*q_seq_len/sequences,
//                                  attn_batches*q_seq_len,
//                                  stream);
//   }
//
//
//   // Matmul1 Dgrad1
//
//   TORCH_CUDABLAS_CHECK(cublasGemmStridedBatchedEx(handle,
//                              CUBLAS_OP_N,
//                              CUBLAS_OP_N,
//                              head_dim,    // m
//                              q_seq_len,   // n
//                              k_seq_len,   // k
//                              static_cast<const void*>(&scale),
//                              static_cast<const void*>(k_lin_results_ptr),  // A:
//                              CUDA_R_16F,
//                              lead_dim,  // lda
//                              batch_stride, // stride A
//                              static_cast<const void*>(matmul2_grads.data_ptr()),
//                              CUDA_R_16F,
//                              k_seq_len,
//                              k_seq_len*q_seq_len,
//                              static_cast<const void*>(&beta),
//                              static_cast<void*>(q_lin_grads_ptr), // C
//                              CUDA_R_16F,
//                              lead_dim,
//                              batch_stride,
//                              attn_batches,
//                              CUDA_R_32F,
//                              CUBLAS_GEMM_DEFAULT_TENSOR_OP));
//
//   // Matmul1 Dgrad2
//
//   TORCH_CUDABLAS_CHECK(cublasGemmStridedBatchedEx(handle,
//                              CUBLAS_OP_N,
//                              CUBLAS_OP_T,
//                              head_dim,    // m
//                              k_seq_len,   // n
//                              q_seq_len,   // k
//                              static_cast<const void*>(&scale),
//                              static_cast<const void*>(q_lin_results_ptr),  // A:
//                              CUDA_R_16F,
//                              lead_dim,  // lda
//                              batch_stride, // stride A
//                              static_cast<const void*>(matmul2_grads.data_ptr()),
//                              CUDA_R_16F,
//                              k_seq_len,
//                              k_seq_len*q_seq_len,
//                              static_cast<const void*>(&beta),
//                              static_cast<void*>(k_lin_grads_ptr), // C
//                              CUDA_R_16F,
//                              lead_dim,
//                              batch_stride,
//                              attn_batches,
//                              CUDA_R_32F,
//                              CUBLAS_GEMM_DEFAULT_TENSOR_OP));
//
//   // Input Linear Dgrad
//   TORCH_CUDABLAS_CHECK(cublasGemmEx(handle,
//                              CUBLAS_OP_N,
//                              CUBLAS_OP_N,
//                              embed_dim,
//                              batches,
//                              output_lin_dim,
//                              static_cast<const void*>(&alpha),
//                              static_cast<const void*>(input_weights.data_ptr()),
//                              CUDA_R_16F,
//                              embed_dim,
// 			                 static_cast<const void*>(input_lin_output_grads.data_ptr()),
//                              //static_cast<const void*>(q_lin_grads_ptr),
//                              CUDA_R_16F,
//                              output_lin_dim,
//                              static_cast<const void*>(&beta),
//                              static_cast<void*>(input_grads.data_ptr()),
//                              CUDA_R_16F,
//                              embed_dim,
//                              CUDA_R_32F,
//                              //CUBLAS_GEMM_ALGO10_TENSOR_OP));
//                              CUBLAS_GEMM_DEFAULT_TENSOR_OP));
//
//   // Input Linear Wgrad
// //   TORCH_CUDABLAS_CHECK(cublasGemmEx(handle,
// //                              CUBLAS_OP_N,
// //                              CUBLAS_OP_T,
// //                              embed_dim,
// //                              output_lin_dim,
// //                              batches,
// //                              static_cast<const void*>(&alpha),
// //                              static_cast<const void*>(inputs.data_ptr()),
// //                              CUDA_R_16F,
// //                              embed_dim,
// //                              static_cast<const void*>(q_lin_grads_ptr),
// //                              CUDA_R_16F,
// //                              output_lin_dim,
// //                              static_cast<const void*>(&beta),
// //                              static_cast<void*>(input_weight_grads.data_ptr()),
// //                              CUDA_R_16F,
// //                              embed_dim,
// //                              CUDA_R_32F,
// //                              CUBLAS_GEMM_DEFAULT_TENSOR_OP));
// //
// //   auto  input_bias_grads = input_lin_output_grads.view({-1, output_lin_dim}).sum(0, false);
//   TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));
//
//   return input_grads;
// }

} // end namespace cublas_gemmex
} // end namespace self
} // end namespace multihead_attn