import numpy as np
import torch, math
import torch.nn as nn
import torch.nn.functional as F
from onmt.modules.Transformer.Layers import PositionalEncoding
from onmt.modules.Transformer.Layers import EncoderLayer, DecoderLayer
from onmt.modules.StochasticTransformer.Layers import StochasticEncoderLayer, StochasticDecoderLayer
from onmt.modules.Transformer.Models import TransformerEncoder, TransformerDecoder
import onmt
from onmt.modules.WordDrop import embedded_dropout
from onmt.modules.Transformer.Layers import XavierLinear, MultiHeadAttention, FeedForward, PrePostProcessing
Linear = XavierLinear

import copy
"""
    Variational Inference for model depth generation

    Our model structure is generated by a latent variable z = {z_1, z_2 .... z_n} corresponding to n layers
    Assumption is each layer is generated randomly (motivated by the Stochastic Network)
    Mean Field assumption is used (one set of parameters for each z)

    Our loss function is:

    L = E_q_z ( log (p (Y|X, z)) - KL( q(z|X, y) || p(z|X))
    (data likelihood given the latent variable)
    
    The Prior model estimates p(z | x)

    The Posterior model estimates q(z | x, y)

    During training we take the sample from posterior (variational inference)
    During testing  y is not available, so we use the prior (conditional prior)

"""

def mean_with_mask(context, mask):

    # context dimensions: T x B x H
    # mask dimension: T x B x 1 (with unsqueeze)
    # first, we have to mask the context with zeros at the unwanted position

    eps = 0
    cont = context.clone()

    context.masked_fill_(mask, eps)
    # then take the sum over the time dimension
    context_sum = torch.sum(context, dim=0, keepdim=False)

    nonzeros = 1 - mask.type_as(context_sum)
    weights = torch.sum(nonzeros, dim=0, keepdim=False)

    mean = context_sum.div_(weights)

    return mean

"""
    The Prior model estimates p(z | x)
"""
class NeuralPrior(nn.Module):

    """Encoder in 'Attention is all you need'
    
    Args:
        opt: list of options ( see train.py )
        dicts : dictionary (for source language)
        
    """
    def __init__(self, opt, embedding, positional_encoder):
    
        super(NeuralPrior, self).__init__()

        encoder_opt = copy.deepcopy(opt)
        # quick_hack to override some hyper parameters of the prior encoder
        encoder_opt.layers = opt.layers if opt.var_ignore_source else opt.layers // 2
        self.dropout = opt.dropout
        self.opt = opt

        self.var_ignore_first_source_token = opt.var_ignore_first_source_token

        self.encoder = TransformerEncoder(encoder_opt, embedding, positional_encoder)

        self.projector = Linear(opt.model_size, opt.model_size)
        self.mean_predictor = Linear(opt.model_size, opt.model_size)
        self.var_predictor = Linear(opt.model_size, opt.model_size)

    def forward(self, input, **kwargs):
        """
        Inputs Shapes: 
            input: batch_size x len_src (wanna tranpose)
        
        Outputs Shapes:
            out: batch_size x len_src x d_model
            mask_src 
            
        """
        if self.var_ignore_first_source_token:
            input = input[:,1:]
        # pass the input to the transformer encoder (we also return the mask)
        freeze_embedding_ = True
        if self.opt.var_ignore_source:
            freeze_embedding_ = False
        context, _ = self.encoder(input, freeze_embedding=freeze_embedding_)
          
        # Now we have to mask the context with zeros
        # context size: T x B x H
        # mask size: T x B x 1 for broadcasting
        mask = input.eq(onmt.Constants.PAD).transpose(0, 1).unsqueeze(2)

        context = mean_with_mask(context, mask)
        encoder_meaning = context
        context = torch.tanh(self.projector(context))
       

        mean = self.mean_predictor(context)       
        log_var = self.var_predictor(context).float()

        var = torch.exp(0.5*log_var)

        p_z = torch.distributions.normal.Normal(mean.float(), var)


        # return prior distribution P(z | X)
        return encoder_meaning, p_z


class NeuralPosterior(nn.Module):

    """Neural Posterior using Transformer
    
    Args:
        opt: list of options ( see train.py )
        embedding : dictionary (for target language)
        
    """
    
    def __init__(self, opt, embedding, positional_encoder, prior=None):
    
        super(NeuralPosterior, self).__init__()
        
        encoder_opt = copy.deepcopy(opt)
        self.opt = opt

        # quick_hack to override some hyper parameters of the prior encoder
        encoder_opt.layers = opt.layers if opt.var_ignore_source else opt.layers // 2
        # encoder_opt.word_dropout = 0.0 
        self.dropout = opt.dropout

        self.var_ignore_first_target_token = opt.var_ignore_first_target_token

        self.posterior_combine = opt.var_posterior_combine
        if opt.var_posterior_combine == 'concat':
            self.projector = Linear(opt.model_size * 2, opt.model_size)
        elif opt.var_posterior_combine == 'sum':
            self.projector = Linear(opt.model_size * 1, opt.model_size)
        else:
            raise NotImplementedError

        if opt.var_posterior_share_weight == True:
            assert prior is not None
            self.encoder = prior.encoder
        else:
            self.encoder = TransformerEncoder(encoder_opt, embedding, positional_encoder)

        self.mean_predictor = Linear(opt.model_size, opt.model_size)
        self.var_predictor = Linear(opt.model_size, opt.model_size)
    
    def forward(self, encoder_meaning, input_src, input_tgt, **kwargs):
        """
        Inputs Shapes: 
            input: batch_size x len_src (wanna tranpose)
        
        Outputs Shapes:
            out: batch_size x len_src x d_model
            mask_src 
            
        """

        """ Embedding: batch_size x len_src x d_model """

        if self.var_ignore_first_target_token:
            input_tgt = input_tgt[:,1:]
        
        # encoder_context = encoder_context.detach()
        decoder_context, _ = self.encoder(input_tgt, freeze_embedding=True)

        # src_mask = input_src.eq(onmt.Constants.PAD).transpose(0, 1).unsqueeze(2)
        tgt_mask = input_tgt.eq(onmt.Constants.PAD).transpose(0, 1).unsqueeze(2) 


        # take the mean of each context
        encoder_context = encoder_meaning
        decoder_context = mean_with_mask(decoder_context, tgt_mask)

        if self.posterior_combine == 'concat':
            context = torch.cat([encoder_context, decoder_context], dim=-1)
        elif self.posterior_combine == 'sum':
            context = encoder_context + decoder_context

        context = torch.tanh(self.projector(context))

        mean = self.mean_predictor(context)
        log_var = self.var_predictor(context).float()
        var = torch.exp(0.5 * log_var)

        q_z = torch.distributions.normal.Normal(mean.float(), var)

        # return distribution Q(z | X, Y)
        return q_z
